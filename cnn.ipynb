{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 5.959939\n",
      "Train Epoch: 1 [640/50000 (1%)]\tLoss: 4.105010\n",
      "Train Epoch: 1 [1280/50000 (3%)]\tLoss: 4.405999\n",
      "Train Epoch: 1 [1920/50000 (4%)]\tLoss: 4.673874\n",
      "Train Epoch: 1 [2560/50000 (5%)]\tLoss: 3.551017\n",
      "Train Epoch: 1 [3200/50000 (6%)]\tLoss: 3.492113\n",
      "Train Epoch: 1 [3840/50000 (8%)]\tLoss: 3.150855\n",
      "Train Epoch: 1 [4480/50000 (9%)]\tLoss: 3.174090\n",
      "Train Epoch: 1 [5120/50000 (10%)]\tLoss: 3.124091\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Training settings\n",
    "# for terminal use. In notebook, you can't parse arguments\n",
    "class args:\n",
    "    cuda = False\n",
    "    batch_size = 64\n",
    "    test_batch_size = 1000\n",
    "    epochs = 10\n",
    "    lr = 0.01\n",
    "    momentum = 0.5\n",
    "    no_cuda = False\n",
    "    seed = 1\n",
    "    log_interval = 10\n",
    "    # if add Dropout\n",
    "    with_dropout = True\n",
    "    # if initialize weights\n",
    "    with_init_weights = True\n",
    "    # if add BatchNorm\n",
    "    with_batchnorm = False\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "# The output of torchvision datasets are PILImage images of range [0, 1].\n",
    "# We transform them to Tensors of normalized range [-1, 1]\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                             ])\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=args.test_batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # TODO: define your network here\n",
    "        self.conv_1 = nn.Conv2d(3, 6, kernel_size=5, stride=1)\n",
    "        if args.with_batchnorm:\n",
    "            self.block_conv_1 = nn.Sequential(\n",
    "                self.conv_1,\n",
    "                nn.BatchNorm2d(6),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "            self.block_conv_2 = nn.Sequential(\n",
    "                nn.Conv2d(6, 16, kernel_size=5, stride=1),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        else:\n",
    "            self.block_conv_1 = nn.Sequential(\n",
    "                self.conv_1,\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "            self.block_conv_2 = nn.Sequential(\n",
    "                nn.Conv2d(6, 16, kernel_size=5, stride=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        if args.with_dropout:\n",
    "            # TODO: replace fc with conv\n",
    "            self.block_fc_1 = nn.Sequential(\n",
    "                nn.Linear(16 * 25, 120),\n",
    "                nn.BatchNorm1d(120),\n",
    "                nn.Dropout()\n",
    "            )\n",
    "            # TODO: replace fc with conv\n",
    "            self.block_fc_2 = nn.Sequential(\n",
    "                nn.Linear(120, 84),\n",
    "                nn.BatchNorm1d(84),\n",
    "                nn.Dropout()\n",
    "            )\n",
    "        else:\n",
    "            self.block_fc_1 = nn.Linear(16*25, 120)\n",
    "            self.block_fc_2 = nn.Linear(120, 84)\n",
    "        # TODO: replace fc with conv\n",
    "        self.fc_3 = nn.Linear(84, 10)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        # Initialize parameters\n",
    "        if args.with_init_weights:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                    m.weight.data.normal_(0, math.sqrt(2. /n))\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.zero_()\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    n = m.out_features\n",
    "                    m.weight.data.normal_(0, math.sqrt(2. /n))\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.zero_()\n",
    "                if args.with_batchnorm and (isinstance(m, nn.BatchNorm1d) or isinstance(m, nn.BatchNorm2d)):\n",
    "                    m = m\n",
    "                    m.weight.data.fill_(1)\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        x = self.block_conv_1(x)\n",
    "        x = self.block_conv_2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.block_fc_1(x)\n",
    "        x = self.block_fc_2(x)\n",
    "        x = self.fc_3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Feature extractor for filter visualization\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model, layer_names):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self._model = model\n",
    "        self._layer_names = set(layer_names)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = dict()\n",
    "        # _modules is an OrderedDict, which replace iteritems() with items() in python3.* \n",
    "        for name, module in self._model._modules.items():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                x = x.view(x.size(0), -1)\n",
    "            x = module(x)\n",
    "            if name in self._layer_names:\n",
    "                out[name] = x\n",
    "        return out\n",
    "\n",
    "# Vesualize training results and trained filters\n",
    "class VisualizedResult():\n",
    "    def __init__(self, model):\n",
    "        self._model = model\n",
    "    def training_curve(self, epoches, train_loss_records, test_loss_records):\n",
    "        fig = plt.figure()\n",
    "        ax_train = fig.add_subplot(111)\n",
    "        ax_test = fig.add_subplot(111)\n",
    "        plt.axis([1, epoches, 0, math.ceil(max(train_loss_records + test_loss_records) * 1.2)])\n",
    "        plt.xlabel('Epoches')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Curve')\n",
    "        plt.plot(range(1, epoches + 1), train_loss_records, 'b-', label=\"train loss\")\n",
    "        plt.plot(range(1, epoches + 1), test_loss_records, 'r-', label=\"test loss\")\n",
    "        for xy in zip(range(1, epoches + 1), train_loss_records):\n",
    "            ax_train.annotate('%.2f' % xy[1], xy=xy, textcoords='data')\n",
    "        for xy in zip(range(1, epoches + 1), test_loss_records):\n",
    "            ax_test.annotate('%.2f' % xy[1], xy=xy, textcoords='data')\n",
    "        plt.legend(loc='upper right', borderaxespad=0.)\n",
    "        plt.show()\n",
    "    def accuracy_curve(self, epoches, accuracy_records):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.axis([1, epoches, 0, 100])\n",
    "        plt.xlabel('Epoches')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy Curve')\n",
    "        plt.plot(range(1, epoches + 1), accuracy_records, '-')\n",
    "        for xy in zip(range(1, epoches + 1), accuracy_records):\n",
    "            ax.annotate('%s%%' % xy[1], xy=xy, textcoords='data')\n",
    "        plt.show()\n",
    "    def conv_filter(self, layer_names):\n",
    "        model.eval()\n",
    "        feature_extractor = FeatureExtractor(self._model, layer_names)\n",
    "        for data, target in test_loader:\n",
    "            if args.cuda:\n",
    "                data = data.cuda()\n",
    "            data = Variable(data, volatile=True)\n",
    "        out = feature_extractor.forward(data)\n",
    "        print(out)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "# TODO: other optimizers\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "train_loss_records = list()\n",
    "test_loss_records = list()\n",
    "accuracy_records = list()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)   # is it true to use such a loss over cross-entropy loss? \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0]\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "    # Average training loss for this epoch\n",
    "    train_loss_records.append(train_loss / len(train_loader))\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss = test_loss\n",
    "    test_loss /= len(test_loader) # loss function already averages over batch size\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "    test_loss_records.append(test_loss)\n",
    "    accuracy_records.append(accuracy)\n",
    "\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "\n",
    "visual_result = VisualizedResult(model)\n",
    "# Visualize training curve\n",
    "visual_result.training_curve(args.epochs, train_loss_records, test_loss_records)\n",
    "# Visualize test accuracy\n",
    "visual_result.accuracy_curve(args.epochs, accuracy_records)\n",
    "# Visualize trained filter on the 1st Conv layer\n",
    "visual_result.conv_filter(['conv_1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
